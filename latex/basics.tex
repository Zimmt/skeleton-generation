%-------------------
%-------------------
\chapter{Grundlagen}
%  Vergleich von Techniken, was gibt es alles, was wurde am Ende verwendet und warum

%--------------------------------
\section{Prozedurale Generierung}

Werden Inhalte, wie Texturen oder 3D-Objekte, generiert, ohne dass diese vor der Ausführung des Algorithmus festgelegt wurden, so wird dies als \emph{prozedurale Generierung} (PG) bezeichnet.
Ursprünglich wurde PG verwendet, weil der Speicherplatz auf Computern sehr begrenzt war. Große 3D-Landschaften oder andere vorgefertigte künstlerische Werke konnten nicht gespeichert werden.\\
Die Demoszene entstand in den 1980er Jahren und treibt PG ins Extreme. Sie zeigt, dass beeindruckende Ergebnisse trotz stark limitiertem Speicherplatz möglich sind \bzw wie man das volle Potential von Computerhardware ausschöpft  \cite{DemoScene}. \todo{mehr beschreiben}

Auf heutigen Rechnern ist der Speicherplatz nicht mehr so begrenzt und es kann viel Inhalt für \zb PC-Spiele von Künstlern vorgefertigt werden. Trotzdem behält PG ihre Daseinsberechtigung, denn der Entwurf komplexer Inhalte, wie beispielsweise Landschaften mit viel Vegetation, ist aufwändig und benötigt viel Zeit. Hier können wieder Algorithmen aus der PG eingesetzt werden um Zeit und Arbeit zu sparen. Sie können sowohl verwendet werden um fertige 3D-Modelle zu generieren, als auch um als Inspiration und Hilfe für den Künstler zu dienen. \emph{SpeedTree} \cite{SpeedTree} ist ein Beispiel für Software, die für die Generierung von Vegetation verwendet wird. Sie unterstützt und beschleunigt die Erstellung von Szenen mit viel Vegetation. \cite{PCGSurvey_videoGames}

Dinge, die oft prozedural generiert werden, sind Landschaften, Straßennetze, Gebäude, Menschen, Tiere oder auch Geschichten \cite{PCGSurvey}. Humanoide Charaktere für Computerspiele werden \zb in der Masterarbeit \cite{ProceduralCharacterGeneration} erzeugt. Die Zusammenstellung \cite{PCGSurvey_videoGames} untersucht PG von Wesen, die Videospiele bevölkern. Obwohl einige Arbeiten genannt werden, die sich mit diesem Theme beschäftigen, wird auch festgestellt, dass dieser Themenbereich noch wenig erforscht ist. Das lässt viel Spielraum für diese Arbeit.

\todo{perlin noise (nicht ausreichend für Skelette, mehr Einschränkungen nötig sonst würden nur Monster generiert) + weitere Methodenbeispiele}


%-------------------------------------------
\section{Animation, Physiksimulationen, IK}

\begin{itemize}
 \item Knochen in Hierarchie (Baum)
 \item keyframes, motion capture (traditional keyframed workflow (sporeanim S.3), wo kann hier Algorithmus verwendet werden?)
 \item rag doll
 \item Skelett, Muskeln, Haut, Faszien (für lebensecht wirkende Tiere)
 \item Featherstone Algorithmus
 \item Drehmomente ausrechnen um Gleichgewicht auszurechnen (hier? oder im Kapitel zu ersten Ansätzen lassen?)
 \item Animation von vorher unbekannten Kreaturen (sporeanim)
 \item skeleton-based design tool (Yamamoto et al 2011): rudimentäres Skelett/Rig für schnelle Produktion und leichte Änderung an Charakteren
        \url{https://dl.acm.org/doi/10.1145/2073304.2073316}, \url{https://www.youtube.com/watch?v=yhq1aUp8QLY}
\end{itemize}

% IK
Inverse Kinematik wird unter anderem in der Robotik verwendet. Gegeben ist eine Kette von Gelenken, die über starre Verbindungsteile miteinander verbunden sind. Für ein oder mehrere Punkte auf dieser Kette können Zielpunkte im Raum angegeben werden. Das Ziel ist es dann eine Konfiguration der Gelenke zu berechnen, so dass alle Zielpunkte erreicht werden.\\
Es gibt viele verschiedene Methoden Probleme aus der inversen Kinematik zu lösen. Diese finden jeweils in Abhängigkeit von der konkreten Problemstellung ihre Anwendung. Ein Überblick über die verschiedenen Techniken ist in Zusammenstellung \cite{IKSurvey} zu finden.

\begin{itemize}
 \item IK für die Animation von Beinen: Girard and Maciejewski 1985
 \item IK die gelernte Posen bevorzugt: style-based inverse kinematics (\url{https://grail.cs.washington.edu/projects/styleik/})
\end{itemize}



%--------------------
\section{Grammatiken}
\label{grammars}

Eine \emph{Grammatik} ist ein Tupel $G = (\Sigma, V, S, P)$ mit 
\begin{itemize}
 \item dem endlichen Alphabet $\Sigma$ der Terminalsymbole
 \item dem endlichen Alphabet $V$ der Nichtterminalsymbole
 \item dem Startsymbol $S \in V$ und
 \item der Menge von Produktionen $P \subseteq (V \cup \Sigma)^* V (V \cup \Sigma)^* \times (V \cup \Sigma)^*$\\ der Form $P \subseteq (V \cup \Sigma)^* V (V \cup \Sigma)^* \rightarrow (V \cup \Sigma)^*$
\end{itemize}

Eine Teilmenge der Grammatiken bilden \emph{kontextsensitive} Grammatiken. Ihre Produktionen haben die Form
\[\alpha A \beta \rightarrow \alpha \gamma \beta \text{ oder } S \rightarrow \epsilon\] \[\text{ mit } A \in V,~ \alpha, \beta, \gamma \in ((V \setminus \{ S \}) \cup \Sigma)^*,~ \gamma \neq \epsilon \text{ und dem leeren Wort } \epsilon.\]

Wiederum eine Teilmenge davon bilden \emph{kontextfreie} Grammatiken mit Produktionen der Form 
\[A \rightarrow w \text{ mit } A \in V \text{ und } w \in (\Sigma \cup V)^*.\] \cite[Abschnitte 1.4 und 1.5]{FormalLanguageTheory}

\emph{L-Systeme} oder Lindenmayer-Systeme sind spezielle Grammatiken. Sie wurden 1968 von dem Biologen Aristid Lindenmayer vorgestellt, mit dem Zweck das Wachstum vielzelliger Lebewesen zu modellieren. Im Unterschied zu Grammatiken, bei denen Produktionen einzeln nacheinander angewendet werden, werden bei L-Systemen alle Symbole gleichzeitig ersetzt. Dies soll modellieren, dass viele Zellteilungen zur gleichen Zeit stattfinden können.\\
Es gibt auch hier wieder verschiedene Teilmengen wie \zb kontextfreie und kontextsensitive L-Systeme. Um Verzweigungen zu modellieren, wurden zusätzlich Klammerstrukturen eingeführt. Das Teilwort innerhalb einer Klammer repräsentiert einen "`Ast"', der wiederum in sich verzweigt sein kann.\\
Oft werden L-Systeme verwendet um das Wachstum von Pflanzen zu modellieren. Es gibt aber auch andere Anwendungsbeispiele, wie \zb die Generierung von Städten \cite{cityGeneration}. \cite[Kapitel 1]{AlgorithmicBeautyOfPlants}



%------------
\section{PCA} % a priori algorithmus?
\label{PCA}

 \todo{Pseudocode?}
 
 % Ziel
 \emph{Principal Component Analysis} (PCA) oder auch Hauptkomponentenanalyse \cite{PCA} wird meist mit dem Ziel angewendet die Dimensionalität einer Menge von Datenpunkten zu verringern, dabei aber möglichst wenig Information zu verlieren.
 So wie beispielsweise in \cite{PCA_faces} bei 3D-Modellen von Gesichtern oder in \cite{PCA_bodies} bei 3D-Modellen von menschlichen Körpern. Auch im Zusammenhang mit prozeduraler Generierung wird PCA oft verwendet, \zb zur Generierung von humanoiden Charakteren für Videospiele \cite{ProceduralCharacterGeneration} oder Texturen für Gesichter \cite{GeneratingFacialTextures}.
 
 Als Ausgangspunkt für eine PCA dient eine Menge von Datenpunkten (oder Beispielen) mit jeweils $n$ Dimensionen. Voraussetzung ist, dass die Punkte in jeder Dimension normal-/gaußverteilt sind. Dann liegen die Punkte im $n$-dimensionalen Raum in einem Ellipsoid.
 
 Nun ist das Ziel herauszufinden wo die Achsen des Ellipsoids liegen. Entlang der Richtungen der Achsen sind die Beispiele widerum normalverteilt und die Verteilungen sind jeweils unabhängig voneinander.
 Jetzt können die Datenpunkte vom Eingabekoordinatensystem in dasjenige Koordinatensystem transformiert werden, das von den Achsen des Ellipsoids aufgespannt wird. Interessant sind dabei die Achsen in deren Richtung die Daten die größe Streuung aufweisen, die "`principal components"'. 
 
 Oft ist das Ziel die Dimensionalität der Eingabedaten zu reduzieren. Das wird dadurch erreicht, dass die Eingabedaten nur noch durch die "`principal components"' dargestellt werden und die anderen Dimensionen weggelassen werden. Je weniger Streuung die Datenpunkte auf denjenigen Achsen aufweisen, die weggelassen werden, desto weniger Information wird verworfen.
 
 Will man stattdessen einen zufälligen Datenpunkt erzeugen, der die gleichen Verteilungen aufweist, wie die Eingabebeispiele, so funktioniert das im transformierten Koordinatensystem ebefalls sehr gut. Da die einzelnen Dimensionen unabhängig voneinander sind, kann man einfach für jede Dimension eine normalverteilte Zufallszahl mit der entsprechenden Varianz ziehen.
 
 Der Mittelpunkt des Ellipsoids ist der Mittelwert der Daten.
 Um die Achsen des Ellipsoids zu berechnen, wird zunächst eine Kovarianzmatrix aufgestellt. Ihre Einträge sind die Kovarianzen aller möglichen Kombinationen der Eingabedimensionen. Von dieser Matrix werden dann alle Eigenvektoren mit zugehörigen Eigenwerten berechnet.
 Die Eigenvektoren sind die Achsen des Ellipsoids und die Eigenwerte geben an wie groß die Varianz entlang dieser Achse ist.
 
 Um nun die Haupteigenschaften oder "`principal components"' eines Datenpunktes zu finden, stellt man ihn im Koordinatensystem des Ellipsoids dar, also als gewichtete Summe der Eigenvektoren. Dann betrachtet man die Dimensionen mit den größten Eigenwerten. Dazu zieht man zunächst den Mittelwert vom Datenpunkt ab und multipliziert ihn mit der transponierten Basiswechselmatrix. Das ist diejenige Matrix, in deren Zeilen die Eigenvektoren der Kovarianzmatrix stehen.

 
%---------------------------------- 
\section{Quantil-Quantil-Diagramme} 
\label{qqdiagrams}

 \begin{itemize}
  \item QQ Diagramme erklären
  \item negative Werte sind ganz normal, da inverse CDF bei einem Mittelwert von 0 bei Werten < 0.5 negativ ist
  \item welche Methode um Unterscheidungsanteil auszurechnen? \url{https://de.wikipedia.org/wiki/Quantil-Quantil-Diagramm} (verschiedene Sachen ausprobieren, zeigen dass egal?
  \item noch genauer: "`Zahl"' ausrechnen (paper)? (oder als Alternative vorschlagen)
 \end{itemize}


%-------------------------------
\section{Genetische Algorithmen}

als Alternative zu Variationen mit PCA

\begin{itemize}
 \item introduction to evolutionary design (bently)
 \item basics on genetic algorithms (Holland 1992) -> bei Bib bestellt
 \item interactive evolutionary computation (Takagi 2001) (fitness function = human evaluation)
 \item jon hudson thesis: creature generation using genetic algorithms \cite{JonHudson}
   \begin{itemize}
    \item relativ simple Regeln, nicht orientiert an spezieller Tierklasse, Einschränkung für Anzahl Beine, für Houdini, kein Skelett sondern 3D-Modell, Rig wird auch generiert
    \item auch Variationen eines Tiers und Crossover zwischen mehreren Exemplaren möglich
   \end{itemize}
  \item Grammar based genetic programming (a survey) 
\end{itemize}


